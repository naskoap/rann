---
title: "Tool Comparison: Optuna vs Hyperopt"
author: "Audrey Bertin, Nasko Apostolov, Nelson Evbarunegbe, & Raymond Li"
format: 
  pdf:
    margin-top: "0.6in"
    margin-bottom: "0.6in"
    include-in-header: 
      text: '\pagenumbering{gobble}'
editor: visual
---

## Introduction

For our project, we will be conducting a comparison of two tools designed to help with the process of **hyperparameter tuning** in a machine learning pipeline.

Hyperparameters are user-selected parameters applied to learning algorithms that affect how they are implemented--for example, constraints, weights, learning rates, number of algorithm layers/complexity, etc. Depending on which set of hyperparameters is selected, the results and performance of a particular machine learning algorithm can vary dramatically. The process of hyperparameter tuning is that of finding a good set of parameters for a specific model and dataset combination.

The possible search space is infinite, making this task challenging. Grid search, in which users pre-define a list of possible parameter combinations and test them one by one, has historically been a common way to do this. However, it's very subjective (completely up to the user to select which parameters they want to try), and can be very time consuming for testing many combinations.

Hyperparameter tools have been developed to assist with this, using advanced algorithms and parallel computing to allow users to test out parameters with a more mathematical and faster approach.

In this project, we are interested in comparing how two separate hyperparameter tuning tools work and perform--both in comparison to each other and to the standard grid search.

These tools are:

-   **Hyperopt:** a popular Python library for hyperparameter optimization that has been around--and been considered industry standard--for more than a decade.

-   **Optuna**: a much newer tool that has some advanced features and is gaining popularity.

Both tools have a UI-based component (either visualizations or a full dashboard) and both have their code publicly available on GitHub:

-   <https://github.com/hyperopt/hyperopt> \|\| <https://github.com/optuna/optuna>

They also both have publicly available documentation sites:

-   <http://hyperopt.github.io/hyperopt/> \|\| <https://optuna.org>

We will evaluate these tools in two separate ways:

1.  A **code review**, where we look at the repositories and note things that they are each doing well or not so well.
2.  An **experimental evaluation**, where we test an actual hyperparameter tuning process using sample data to compare ease of use, UI features, and which tool creates the best final model.

## Code Review

For the code review, we will look at the GitHub pages and documentation sites for each of the tools. We will consider both features involving general repository health, as well as more code-specific items such as non-functional requirements.

For general repository health we will look at the following features of each GitHub repository:

-   **Popularity/Use** -- how many stars and active users does it have?

-   **Community** -- how active is the community (how many contributors are there, pull requests, etc).

-   **Maintenance** - how often are changes made to the main branch? When was the last release? Are issues/pull requests getting handled or left behind?

-   **Dependencies** -- how many software dependencies does each project have?

-   **Security** -- are there any potential security vulnerabilities?

For non-functional requirements, we will look at the following subset:

-   **Understandability** -- how easy is it for someone unfamiliar with the code to understand what is going on?

-   **Testability/Debuggability** - is the code formatted in such a way that it is easy to test features or debug when a problem occurs (modular, throws exceptions, etc)? What does the current test coverage look like?

-   **Extensibility/Scalability** - is it easy to add on new features or contribute to the project (since it's open source)? Are the existing features written/provided in such a way that it is easy to implement the code at scale, such as in some sort of production environment at a company, or are external libraries or new features needed?

For each of these categories we will rate the repository's performance on a scale from 1-10. This score is subjective and will be backed up by arguments for both what the repository is doing well, and where it could be improved. At the end, we will share a table summarizing the ratings in each category to compare each tool's strengths and weaknesses (code-wise). We will also sum up the scores to determine an overall best tool based on code quality.

## Experimental Evaluation

For the experimental version of the tool, we will be comparing three separate options for Hyperparameter tuning: 1) Standard grid search, 2) Using Hyperopt, 3) Using Optuna.

For each tuning technique, we will run a full machine learning tuning process *two* separate times, once for a [classification]{.underline} problem and once for a [regression]{.underline} problem. This means a total of 6 different conditions.

For our [classification]{.underline} condition, we will be using a dataset to predict whether a patient will be readmitted to the hospital based on data from their prior visit (number of days at hospital, medications given, etc.). A sample of some of the columns in this dataset is shown below:

```{r}
#| include: false
library(tidyverse)
library(knitr)
```

```{r}
#| echo: false
readmissions <- readr::read_csv("../data/classification/hospital_readmissions.csv", show_col_types = FALSE)
knitr::kable(readmissions %>% select(age, time_in_hospital, n_lab_procedures,diag_1, glucose_test, readmitted) %>% head(3))
```

We will predict the variable `readmitted` using all of the other variables. We will use the `RandomForestClassifier` from `scikit-learn` as our machine learning model.

In order to compare the performance of each tool on the classification problem, we will use the following metrics:

# NEED METRICS

We will also do a short UI review comparing the UI options in Hyperopt and Optuna for a classification problem.

For our [regression]{.underline} condition, we will be predicting the CO2 emissions of a vehicle based on a variety of features, such as engine size, transmission type, and fuel type. A sample of the dataset (not all columns) is included below:

```{r}
#| echo: false
emissions <- readr::read_csv("../data/regression/CO2_Emissions_Canada.csv", show_col_types = FALSE)
new_cols <- c("make", "model", "class", "engine_size", "cylinders", "transmission", "fuel_type", "fuel_consump_city", "fuel_consump_hwy", "fuel_consump_cmb", "fuel_consump_cmb_mph", "emissions") 



colnames(emissions) <- new_cols
emissions <- emissions %>% select(-fuel_consump_cmb_mph) 


knitr::kable(emissions %>% select(class, engine_size, cylinders, fuel_type, fuel_consump_city, emissions) %>% head(3))

```

We will predict the variable `emissions` using all of the other variables. We will use the `RandomForestRegressor` from `scikit-learn` as our machine learning model.

In order to compare the performance of each tool on the classification problem, we will use the following metrics:

# NEED METRICS

## Related Work/References

We will consider several outside resources as we complete our study:

1.  A [research paper](https://arxiv.org/pdf/2201.06433.pdf) comparing a variety hyperparameter optimization tools as well as describing some mathematical theory behind how they work. This paper discusses both Hyperopt and Optuna.
2.  An [official research paper](https://dl.acm.org/doi/pdf/10.1145/3292500.3330701) written on Optuna.
3.  An [official research paper](https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008/meta?casa_token=tqIORp5s9sEAAAAA:FjY1TKwQHaK4LDiD4VIyEc9GKqaqMQR-mOFO2WhtaxvdBO6m5hdnl8A8xuo-2pwHGFL1nY-f) written on Hyperopt.
4.  [Information](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search) on how to complete a grid search in scikit-learn and [sample code](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) on implementing this with random forest.
5.  Sample code for implementing Optuna and Hyperopt:
    -   [Optuna #1](https://towardsdatascience.com/optuna-a-flexible-efficient-and-scalable-hyperparameter-optimization-framework-d26bc7a23fff)

    -   [Optuna #2](https://towardsdatascience.com/exploring-optuna-a-hyper-parameter-framework-using-logistic-regression-84bd622cd3a5)

    -   [Hyperopt #1](https://www.kaggle.com/code/virajbagal/eda-xgb-random-forest-parameter-tuning-hyperopt/notebook)

    -   [Hyperopt #2](https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/)
